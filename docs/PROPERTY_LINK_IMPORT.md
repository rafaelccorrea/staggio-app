# Importa√ß√£o de Propriedades via Link Externo

## üìã Vis√£o Geral

Esta feature permite que usu√°rios cadastrem propriedades automaticamente ao fornecer um link de um site externo (ex: OLX, ZAP Im√≥veis, QuintoAndar, etc.). O sistema coleta automaticamente imagens, t√≠tulos, informa√ß√µes e preenche o formul√°rio de cadastro.

## üèóÔ∏è Arquitetura

### Fluxo de Funcionamento

```
1. Usu√°rio insere link na p√°gina de cria√ß√£o
   ‚Üì
2. Frontend envia link para o backend
   ‚Üì
3. Backend identifica o site e faz scraping/coleta
   ‚Üì
4. Backend retorna dados estruturados
   ‚Üì
5. Frontend preenche formul√°rio com dados coletados
   ‚Üì
6. Usu√°rio revisa e ajusta dados
   ‚Üì
7. Usu√°rio salva propriedade normalmente
```

## üéØ Componentes

### Frontend

1. **Componente de Importa√ß√£o por Link**
   - Modal ou se√ß√£o no formul√°rio de cria√ß√£o
   - Campo de input para URL
   - Bot√£o para coletar dados
   - Indicador de carregamento
   - Preview dos dados coletados
   - Bot√£o para aplicar dados ao formul√°rio

2. **Servi√ßo de API**
   - `propertyImportApi.ts` - Comunica√ß√£o com backend

3. **Tipos TypeScript**
   - `PropertyImportData` - Dados coletados do link
   - `PropertyImportResponse` - Resposta da API

### Backend (Proposta)

1. **Endpoint de Importa√ß√£o**
   ```
   POST /properties/import-from-link
   Body: { url: string }
   Response: PropertyImportData
   ```

2. **Servi√ßos de Scraping**
   - Identificador de site (OLX, ZAP, etc.)
   - Scrapers espec√≠ficos por site
   - Normaliza√ß√£o de dados
   - Download e armazenamento de imagens

## üìä Estrutura de Dados

### PropertyImportData

```typescript
interface PropertyImportData {
  // Dados b√°sicos
  title?: string;
  description?: string;
  type?: PropertyType; // inferido ou mapeado
  status?: PropertyStatus;
  
  // Localiza√ß√£o
  address?: string;
  street?: string;
  number?: string;
  complement?: string;
  city?: string;
  state?: string;
  zipCode?: string;
  neighborhood?: string;
  
  // Caracter√≠sticas
  totalArea?: number;
  builtArea?: number;
  bedrooms?: number;
  bathrooms?: number;
  parkingSpaces?: number;
  
  // Valores
  salePrice?: number;
  rentPrice?: number;
  condominiumFee?: number;
  iptu?: number;
  
  // Imagens (URLs para download)
  imageUrls?: string[];
  
  // Dados extras do site
  sourceUrl: string;
  sourceSite?: string; // 'olx', 'zap', 'quintoandar', etc.
  rawData?: any; // Dados brutos para refer√™ncia
}
```

## üîß Implementa√ß√£o Sugerida

### Sites Priorit√°rios (Brasil)

1. **OLX** - Mercado Livre Im√≥veis
2. **ZAP Im√≥veis**
3. **QuintoAndar**
4. **Viva Real**
5. **Imovelweb**

### Considera√ß√µes T√©cnicas

1. **CORS e Proxy**
   - Scraping precisa ser feito no backend
   - Considerar proxy para evitar bloqueios
   - Rate limiting para evitar ban

2. **Mapeamento de Dados**
   - Cada site tem estrutura diferente
   - Normalizar dados para formato padr√£o
   - Tratamento de campos faltantes

3. **Imagens**
   - Download de imagens no backend
   - Convers√£o para formato adequado
   - Upload para storage (S3, etc.)

4. **Valida√ß√£o**
   - Validar dados coletados
   - Permitir edi√ß√£o antes de salvar
   - Alertas para dados faltantes cr√≠ticos

## üöÄ Roadmap de Implementa√ß√£o

### Fase 1: Frontend B√°sico
- [ ] Componente de input de link
- [ ] Integra√ß√£o com API
- [ ] Preenchimento autom√°tico do formul√°rio
- [ ] Feedback visual de sucesso/erro

### Fase 2: Backend MVP
- [ ] Endpoint de importa√ß√£o
- [ ] Scraper para 1-2 sites principais (ex: OLX, ZAP)
- [ ] Download b√°sico de imagens
- [ ] Mapeamento de dados

### Fase 3: Melhorias
- [ ] Suporte a mais sites
- [ ] Melhoria na detec√ß√£o de campos
- [ ] Cache de dados coletados
- [ ] Hist√≥rico de importa√ß√µes

### Fase 4: Avan√ßado
- [ ] Suporte a m√∫ltiplos links (batch)
- [ ] IA para melhorar mapeamento
- [ ] Valida√ß√£o inteligente de dados
- [ ] Preview antes de aplicar

## ‚ö†Ô∏è Limita√ß√µes e Desafios

1. **Mudan√ßas nos Sites**
   - Sites podem mudar estrutura HTML
   - Scrapers precisam ser mantidos

2. **Termos de Uso**
   - Verificar ToS de cada site
   - Considerar APIs oficiais quando dispon√≠veis

3. **Performance**
   - Scraping pode ser lento
   - Implementar timeout e retry

4. **Qualidade dos Dados**
   - Dados podem estar incompletos
   - Usu√°rio sempre deve revisar

## üìù Notas Adicionais

- Considerar usar servi√ßos terceiros (ScraperAPI, Apify) para facilitar
- Implementar fallback quando scraping falhar
- Adicionar logs detalhados para debugging
- Permitir importa√ß√£o manual parcial se autom√°tica falhar

